{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46b61c10-fa7b-45fb-804e-d4b39c6c4fca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-04T04:30:54.165506Z",
     "iopub.status.busy": "2022-08-04T04:30:54.164220Z",
     "iopub.status.idle": "2022-08-04T04:31:01.678129Z",
     "shell.execute_reply": "2022-08-04T04:31:01.676696Z",
     "shell.execute_reply.started": "2022-08-04T04:30:54.165459Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘book’: File exists\n",
      "mkdir: cannot create directory ‘data’: File exists\n",
      "Collecting gensim\n",
      "  Downloading gensim-4.2.0-cp39-cp39-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (24.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.0/24.0 MB\u001b[0m \u001b[31m46.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting kaggle\n",
      "  Downloading kaggle-1.5.12.tar.gz (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.0/59.0 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.9/dist-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.9/dist-packages (from gensim) (1.23.1)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.9/dist-packages (from gensim) (1.8.1)\n",
      "Requirement already satisfied: six>=1.10 in /usr/lib/python3/dist-packages (from kaggle) (1.14.0)\n",
      "Requirement already satisfied: certifi in /usr/lib/python3/dist-packages (from kaggle) (2019.11.28)\n",
      "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.9/dist-packages (from kaggle) (2.8.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from kaggle) (2.28.1)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from kaggle) (4.64.0)\n",
      "Requirement already satisfied: python-slugify in /usr/lib/python3/dist-packages (from kaggle) (4.0.0)\n",
      "Requirement already satisfied: urllib3 in /usr/local/lib/python3.9/dist-packages (from kaggle) (1.26.10)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /usr/local/lib/python3.9/dist-packages (from requests->kaggle) (2.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->kaggle) (2.8)\n",
      "Building wheels for collected packages: kaggle\n",
      "  Building wheel for kaggle (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kaggle: filename=kaggle-1.5.12-py3-none-any.whl size=73031 sha256=f5a624bb7e4381ca8ecd2daa42a0baade94b3fe1ac93b7941c7daa761a6c085b\n",
      "  Stored in directory: /root/.cache/pip/wheels/ac/b2/c3/fa4706d469b5879105991d1c8be9a3c2ef329ba9fe2ce5085e\n",
      "Successfully built kaggle\n",
      "Installing collected packages: kaggle, gensim\n",
      "Successfully installed gensim-4.2.0 kaggle-1.5.12\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!mkdir book\n",
    "!mkdir data\n",
    "!pip install gensim kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8c8176-ffc1-4ad4-a015-9822e2817773",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-04T04:21:40.857514Z",
     "iopub.status.busy": "2022-08-04T04:21:40.857170Z"
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import glob\n",
    "import string\n",
    "import os\n",
    "import codecs\n",
    "\n",
    "BASE_BOOK_URL = 'https://www.gutenberg.org/browse/scores/top'\n",
    "\n",
    "html = requests.get(BASE_BOOK_URL).text\n",
    "soup = BeautifulSoup(html)\n",
    "\n",
    "unq_code = {}\n",
    "for s in soup.findAll('li'):\n",
    "    url = s.a['href']\n",
    "    if 'ebooks' in url:\n",
    "        url_str = url.split('/')[-1]\n",
    "        if url_str!='':\n",
    "            unq_code[url.split('/')[-1]] = s.a.text\n",
    "        \n",
    "BOOK_TXT_BASE = 'https://www.gutenberg.org/files/'\n",
    "book_urls = []\n",
    "for code in unq_code:\n",
    "    book_urls.append(os.path.join(BOOK_TXT_BASE,f'{code}/{code}-0.txt'))\n",
    "\n",
    "for b in book_urls:\n",
    "    name = b.split('/')[-2]\n",
    "    html = requests.get(b).text\n",
    "    with codecs.open(f'book/{name}.txt', 'w', 'utf-8') as infile:\n",
    "        infile.write(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f085db2e-a1d1-4aa1-b2e8-625758e05410",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting Glossary and Definitions from GradeSaver for novels extracted from Project Gutenberg\n",
    "\n",
    "BASE_GLOSS_URL = 'https://www.gradesaver.com/'\n",
    "TERMINAL = '/study-guide/glossary-of-terms'\n",
    "\n",
    "def punctuations(data_str):\n",
    "    data_str = data_str.replace(\"'s\", \"\")\n",
    "    for x in data_str.lower():\n",
    "        if x in string.punctuation: \n",
    "            data_str = data_str.replace(x, \"\")\n",
    "    return data_str\n",
    "\n",
    "for book in glob.glob('book/*'):\n",
    "    code = book.split('/')[-1].split('.')[0]\n",
    "\n",
    "    try:\n",
    "        bookname = unq_code[code]\n",
    "        bookname = bookname.split(' by ')[0].lower()\n",
    "        bookname = punctuations(bookname)\n",
    "        bookname = bookname.replace(\" \", \"-\")\n",
    "        html = requests.get(BASE_GLOSS_URL+bookname+TERMINAL).text\n",
    "        soup = BeautifulSoup(html)\n",
    "        tt = []\n",
    "        for term in soup.findAll(\"section\", {\"class\": \"linkTarget\"}):\n",
    "            tt.append(\n",
    "            \t\t[term.h2.text.lower().strip(),\n",
    "                \tterm.p.text.lower().strip()]\n",
    "                )\n",
    "        if len(tt):\n",
    "            print (f'Done: {bookname}')\n",
    "            data = pd.DataFrame(tt, columns=['word', 'def'])\n",
    "            data.to_csv(f'data/{code}.csv', \\\n",
    "                                sep='\\t', \\\n",
    "                                encoding='utf-8', \\\n",
    "                                index=False)\n",
    "        else:\n",
    "            print (f'Skipped: {bookname}')\n",
    "    except Exception as e: print (e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20d3bf4a-8192-4cc0-9102-3cc253dc312d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-04T04:36:07.479680Z",
     "iopub.status.busy": "2022-08-04T04:36:07.479177Z",
     "iopub.status.idle": "2022-08-04T04:37:07.565628Z",
     "shell.execute_reply": "2022-08-04T04:37:07.564856Z",
     "shell.execute_reply.started": "2022-08-04T04:36:07.479653Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>word</th>\n",
       "      <th>def</th>\n",
       "      <th>firstcontext</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>abominable</td>\n",
       "      <td>detestable, awful</td>\n",
       "      <td>she winked hard , shook her head , and said gr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>aristocratic</td>\n",
       "      <td>belonging to a high social or political class;...</td>\n",
       "      <td>believe marmee would ask that if we were all r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>assiduity</td>\n",
       "      <td>persistent and diligent effort</td>\n",
       "      <td>laughed less than usual , was a little absent-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>belladonna</td>\n",
       "      <td>a medicine made of deadly nightshade, used as ...</td>\n",
       "      <td>with them till he turned round all of a sudden...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>benevolence</td>\n",
       "      <td>inclination to do good and help others</td>\n",
       "      <td>. jo often watched him , trying to discover th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index          word                                                def  \\\n",
       "0      2    abominable                                  detestable, awful   \n",
       "1      3  aristocratic  belonging to a high social or political class;...   \n",
       "2      4     assiduity                     persistent and diligent effort   \n",
       "3      5    belladonna  a medicine made of deadly nightshade, used as ...   \n",
       "4      6   benevolence             inclination to do good and help others   \n",
       "\n",
       "                                        firstcontext  \n",
       "0  she winked hard , shook her head , and said gr...  \n",
       "1  believe marmee would ask that if we were all r...  \n",
       "2  laughed less than usual , was a little absent-...  \n",
       "3  with them till he turned round all of a sudden...  \n",
       "4  . jo often watched him , trying to discover th...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import codecs\n",
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "\n",
    "def get_context(c):\n",
    "    try:\n",
    "        result = text.concordance_list(c)[0]\n",
    "        left_of_query = ' '.join(result.left)\n",
    "        query = result.query\n",
    "        right_of_query = ' '.join(result.right)\n",
    "        return left_of_query + ' ' + query + ' ' + right_of_query\n",
    "    except:\n",
    "        return ''\n",
    "\n",
    "generated_dfs = []\n",
    "BASE_DIR = 'data'\n",
    "for book in glob.glob('book/*'):\n",
    "    book_name = book.split('/')[-1].split('.')[0]\n",
    "    try:\n",
    "        DATA_DIR = codecs.open('book/' + book_name + '.txt', \\\n",
    "        \t\t\t\t\t\t'rb', \\\n",
    "                                encoding='utf-8').readlines()\n",
    "        true_data = pd.read_csv(\n",
    "        \t\t\t'data/'+book_name+'.csv', \\\n",
    "        \t\t\tsep='\\t')\n",
    "        full_data = ' '.join([i.lower().strip() for i in DATA_DIR if len(i.strip())>1])\n",
    "        tokens = nltk.word_tokenize(full_data)\n",
    "        text = nltk.Text(tokens)\n",
    "        true_data['firstcontext'] = true_data['word'].map(lambda k: get_context(k))\n",
    "        generated_dfs.append(true_data)\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "final_df = pd.concat(generated_dfs[:], axis=0)\n",
    "final_df = final_df[final_df['firstcontext']!='']\n",
    "final_df = final_df[['word', 'def', 'firstcontext']].reset_index()\n",
    "final_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b219102c-47d9-4627-a9ef-ee65fde054da",
   "metadata": {},
   "source": [
    "## to load kaggle dataset\n",
    "\n",
    "1. Get a Kaggle account\n",
    "2. Create an API token by going to your Account settings, and save kaggle.json. Note: you may need to create a new api token if you have already created one. \n",
    "3. Upload kaggle.json to this Gradient Notebook\n",
    "4. Either run the cell below or run the following commands in a terminal (this may take a while)\n",
    "\n",
    "> Note: Do not share a notebook with your api key enabled\n",
    "\n",
    "terminal:\n",
    "\n",
    "mkdir ~/.kaggle/\n",
    "\n",
    "mv kaggle.json ~/.kaggle/\n",
    "\n",
    "pip install kaggle\n",
    "\n",
    "kaggle datasets download adarshsng/googlenewsvectors\n",
    "\n",
    "unzip googlenewsvectors.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04b8786a-89a8-4ae8-973a-dfa8b98a7afb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-04T04:23:15.641743Z",
     "iopub.status.busy": "2022-08-04T04:23:15.641484Z",
     "iopub.status.idle": "2022-08-04T04:23:15.646134Z",
     "shell.execute_reply": "2022-08-04T04:23:15.644994Z",
     "shell.execute_reply.started": "2022-08-04T04:23:15.641717Z"
    }
   },
   "outputs": [],
   "source": [
    "# !kaggle datasets download adarshsng/googlenewsvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5c1bda96-2c24-4989-8daa-47504d0f32c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-04T04:47:15.733752Z",
     "iopub.status.busy": "2022-08-04T04:47:15.733354Z",
     "iopub.status.idle": "2022-08-04T04:47:15.741503Z",
     "shell.execute_reply": "2022-08-04T04:47:15.740466Z",
     "shell.execute_reply.started": "2022-08-04T04:47:15.733726Z"
    }
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "import numpy as np\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "\n",
    "# filepath = \"GoogleNews-vectors-negative300.bin\"\n",
    "# wv_from_bin = KeyedVectors.load_word2vec_format(filepath, binary=True) \n",
    "\n",
    "# ei = {}\n",
    "# for word, vector in zip(wv_from_bin.index_to_key, wv_from_bin.vectors):\n",
    "#     coefs = np.asarray(vector, dtype='float32')\n",
    "#     ei[word] = coefs\n",
    "    \n",
    "def avg_feature_vector(sentence, model, num_features):\n",
    "    words = sentence.split()\n",
    "    #feature vector is initialized as an empty array\n",
    "    feature_vec = np.zeros((num_features, ), dtype='float32')\n",
    "    n_words = 0\n",
    "    for word in words:\n",
    "        if word in ei.keys():\n",
    "            n_words += 1\n",
    "            feature_vec = np.add(feature_vec, model[word])\n",
    "    if (n_words > 0):\n",
    "        feature_vec = np.divide(feature_vec, n_words)\n",
    "    return feature_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0e4174b7-34cc-4a68-af30-e4d82c4d0d70",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-04T04:59:59.047777Z",
     "iopub.status.busy": "2022-08-04T04:59:59.047401Z",
     "iopub.status.idle": "2022-08-04T04:59:59.096902Z",
     "shell.execute_reply": "2022-08-04T04:59:59.094743Z",
     "shell.execute_reply.started": "2022-08-04T04:59:59.047749Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32/245845835.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final_df['new_def'][idx]=s_sort[0]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [59]\u001b[0m, in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m     score \u001b[38;5;241m=\u001b[39m similarity(sense_def, fs)\n\u001b[1;32m     22\u001b[0m     s_dic[de]\u001b[38;5;241m=\u001b[39mscore\n\u001b[0;32m---> 23\u001b[0m s_sort \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43ms_dic\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43mk\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mreverse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m     24\u001b[0m final_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnew_def\u001b[39m\u001b[38;5;124m'\u001b[39m][idx]\u001b[38;5;241m=\u001b[39ms_sort[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     25\u001b[0m final_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatch\u001b[39m\u001b[38;5;124m'\u001b[39m][idx]\u001b[38;5;241m=\u001b[39ms_sort[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# from nltk.corpus import wordnet\n",
    "from scipy.spatial import distance\n",
    "# nltk.download('omw-1.4')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "\n",
    "def similarity(s1, s2):\n",
    "    s1_afv = avg_feature_vector(s1, model=ei, num_features=300)\n",
    "    s2_afv = avg_feature_vector(s2, model=ei, num_features=300)\n",
    "    cos = distance.cosine(s1_afv, s2_afv)\n",
    "    return cos\n",
    "for idx in range(final_df.shape[0]):\n",
    "    fs = final_df.iloc[idx]['firstcontext']\n",
    "    w = final_df.iloc[idx]['word']\n",
    "    defi = final_df.iloc[idx]['def']\n",
    "    syns = wordnet.synsets(w)\n",
    "    s_dic={}\n",
    "    for sense in syns:\n",
    "        de,ex = sense.definition(), sense.examples()\n",
    "        sense_def = de + ' '.join(ex)\n",
    "        score = similarity(sense_def, fs)\n",
    "        s_dic[de]=score\n",
    "    s_sort = sorted(s_dic.items(), key=lambda k:k[1],reverse=True)[0]\n",
    "    final_df['new_def'][idx]=s_sort[0]\n",
    "    final_df['match'][idx]=s_sort[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b33c53b-bfdd-4957-a496-6d0ae390bf16",
   "metadata": {},
   "source": [
    "## to load kaggle dataset\n",
    "\n",
    "1. Get a Kaggle account\n",
    "2. Create an API token by going to your Account settings, and save kaggle.json. Note: you may need to create a new api token if you have already created one. \n",
    "3. Upload kaggle.json to this Gradient Notebook\n",
    "4. Either run the cell below or run the following commands in a terminal (this may take a while)\n",
    "\n",
    "> Note: Do not share a notebook with your api key enabled\n",
    "\n",
    "terminal:\n",
    "\n",
    "mkdir ~/.kaggle/\n",
    "\n",
    "mv kaggle.json ~/.kaggle/\n",
    "\n",
    "pip install kaggle\n",
    "\n",
    "kaggle datasets download therohk/urban-dictionary-words-dataset\n",
    "\n",
    "unzip urban-dictionary-words-dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ee0288-1960-4da8-baee-e872224df2e5",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2022-08-04T04:23:54.923138Z",
     "iopub.status.idle": "2022-08-04T04:23:54.923581Z",
     "shell.execute_reply": "2022-08-04T04:23:54.923368Z",
     "shell.execute_reply.started": "2022-08-04T04:23:54.923344Z"
    }
   },
   "outputs": [],
   "source": [
    "# !kaggle datasets download therohk/urban-dictionary-words-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f82e789e-a44e-4ac6-9bfb-7c9f6db6f21b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-04T05:00:02.559764Z",
     "iopub.status.busy": "2022-08-04T05:00:02.559330Z",
     "iopub.status.idle": "2022-08-04T05:00:03.025350Z",
     "shell.execute_reply": "2022-08-04T05:00:03.024151Z",
     "shell.execute_reply.started": "2022-08-04T05:00:02.559714Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_32/3466570859.py:3: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  train = pd.read_csv(\n",
      "b'Skipping line 3692: expected 6 fields, saw 7\\nSkipping line 5546: expected 6 fields, saw 7\\nSkipping line 7198: expected 6 fields, saw 7\\nSkipping line 9758: expected 6 fields, saw 7\\nSkipping line 13350: expected 6 fields, saw 7\\nSkipping line 20000: expected 6 fields, saw 7\\nSkipping line 20088: expected 6 fields, saw 7\\nSkipping line 21776: expected 6 fields, saw 8\\nSkipping line 23826: expected 6 fields, saw 8\\nSkipping line 25255: expected 6 fields, saw 7\\nSkipping line 25643: expected 6 fields, saw 7\\nSkipping line 25777: expected 6 fields, saw 7\\nSkipping line 30965: expected 6 fields, saw 7\\nSkipping line 35485: expected 6 fields, saw 7\\nSkipping line 36022: expected 6 fields, saw 8\\nSkipping line 36072: expected 6 fields, saw 7\\nSkipping line 40152: expected 6 fields, saw 7\\nSkipping line 40695: expected 6 fields, saw 7\\nSkipping line 41942: expected 6 fields, saw 7\\nSkipping line 43660: expected 6 fields, saw 7\\nSkipping line 46529: expected 6 fields, saw 7\\nSkipping line 48482: expected 6 fields, saw 7\\nSkipping line 49277: expected 6 fields, saw 7\\nSkipping line 49718: expected 6 fields, saw 9\\nSkipping line 50662: expected 6 fields, saw 7\\nSkipping line 50899: expected 6 fields, saw 7\\nSkipping line 53871: expected 6 fields, saw 8\\nSkipping line 54199: expected 6 fields, saw 8\\nSkipping line 54595: expected 6 fields, saw 8\\nSkipping line 56867: expected 6 fields, saw 7\\nSkipping line 57140: expected 6 fields, saw 7\\nSkipping line 60471: expected 6 fields, saw 7\\nSkipping line 65130: expected 6 fields, saw 7\\nSkipping line 65934: expected 6 fields, saw 9\\nSkipping line 68114: expected 6 fields, saw 7\\nSkipping line 68537: expected 6 fields, saw 7\\nSkipping line 74771: expected 6 fields, saw 8\\nSkipping line 88345: expected 6 fields, saw 11\\nSkipping line 89570: expected 6 fields, saw 7\\nSkipping line 89989: expected 6 fields, saw 7\\nSkipping line 92650: expected 6 fields, saw 7\\nSkipping line 94928: expected 6 fields, saw 8\\nSkipping line 95130: expected 6 fields, saw 7\\nSkipping line 97923: expected 6 fields, saw 7\\n'\n",
      "/tmp/ipykernel_32/3466570859.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_train['word'] = new_train.word.str.lower()\n",
      "/tmp/ipykernel_32/3466570859.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  new_train['definition'] = new_train.definition.str.lower()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train = pd.read_csv(\n",
    "\t\t  'urbandict-word-defs.csv', \\\n",
    "                    nrows=100000, \\\n",
    "                    error_bad_lines=False\n",
    "                    )\n",
    "\n",
    "new_train = train[['word', 'definition']]\n",
    "new_train['word'] = new_train.word.str.lower()\n",
    "new_train['definition'] = new_train.definition.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ed015448-7649-40fc-8ebd-a30a34d3d8a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-04T05:00:22.395834Z",
     "iopub.status.busy": "2022-08-04T05:00:22.395310Z",
     "iopub.status.idle": "2022-08-04T05:01:06.385417Z",
     "shell.execute_reply": "2022-08-04T05:01:06.384294Z",
     "shell.execute_reply.started": "2022-08-04T05:00:22.395787Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "878f259656074224b9e7e364ffa3ea76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/0.99M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe80bf3d3ad34283865820b467dedca4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd4f46f9f38e45949ccf3798f56803c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/718 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee36b552492042aba703f984c23e6a58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "import logging\n",
    "logging.getLogger().setLevel(logging.CRITICAL)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "    \n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2-medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "5edf4199-e460-45c6-8b8e-6ed8149e3418",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-04T05:01:06.395714Z",
     "iopub.status.busy": "2022-08-04T05:01:06.395313Z",
     "iopub.status.idle": "2022-08-04T05:01:22.971331Z",
     "shell.execute_reply": "2022-08-04T05:01:22.970131Z",
     "shell.execute_reply.started": "2022-08-04T05:01:06.395686Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import os\n",
    "import json\n",
    "import csv\n",
    "\n",
    "class GlossaryDataset(Dataset):\n",
    "    def __init__(self, dataframe):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        self.data_list = []\n",
    "        self.end_of_text_token = \"<|endoftext|>\"\n",
    "        self.start_of_text_token = \"<|startoftext|>\"\n",
    "        \n",
    "        for i in range(dataframe.shape[0]):\n",
    "            data_str = f\"{self.start_of_text_token} \\\n",
    "            \t\t\t{new_train.iloc[i]['word']} \\\n",
    "            \t\t\t<DEFINE> \\\n",
    "            \t\t\t{new_train.iloc[i]['definition']} \\\n",
    "            \t\t\t{self.end_of_text_token}\"\n",
    "            self.data_list.append(data_str)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data_list)\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.data_list[item]\n",
    "        \n",
    "dataset = GlossaryDataset(dataframe=new_train)\n",
    "data_loader = DataLoader(dataset, batch_size=4, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "78f008e6-25da-4289-b078-a6d39fc46e30",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-04T05:05:44.567977Z",
     "iopub.status.busy": "2022-08-04T05:05:44.567607Z",
     "iopub.status.idle": "2022-08-04T05:05:44.589672Z",
     "shell.execute_reply": "2022-08-04T05:05:44.588786Z",
     "shell.execute_reply.started": "2022-08-04T05:05:44.567952Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AdamW \n",
    "\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 2e-5\n",
    "\n",
    "device = 'cpu'\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "\n",
    "model = model.to(device)\n",
    "model.train()\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "lambda1 = lambda epoch: 0.65 ** epoch\n",
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lambda1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0b5c61-740a-4ccf-9c96-46c110fc7197",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-08-04T05:05:45.733573Z",
     "iopub.status.busy": "2022-08-04T05:05:45.733207Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 0 epoch\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(EPOCHS):\n",
    "  print (f'Running {epoch} epoch')\n",
    "\n",
    "  for idx,sample in enumerate(data_loader):\n",
    "    sample_tsr = torch.tensor(tokenizer.encode(sample[0]))\n",
    "    sample_tsr = sample_tsr.unsqueeze(0).to(device)\n",
    "    outputs = model(sample_tsr, labels=sample_tsr)\n",
    "    loss = outputs[0]\n",
    "    loss.backward()\n",
    "       \n",
    "    optimizer.step()\n",
    "    scheduler.step() \n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a5835d-6e56-48ed-8be6-3b789cad93df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
